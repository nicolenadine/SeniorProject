import os
import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
import math
from PIL import Image
import glob
from tqdm import tqdm
import logging
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import seaborn as sns
import gc


class MalwareImageGenerator:
    def __init__(self, w2v_model_path, output_dir="./generated_images", img_size=(256, 256)):
        """
        Initialize the malware image generator

        Args:
            w2v_model_path: Path to the trained word2vec model
            output_dir: Directory to save generated images
            img_size: Size of the output images (width, height)
        """
        self.model = Word2Vec.load(w2v_model_path)
        self.embedding_size = self.model.wv.vector_size
        self.img_size = img_size
        self.output_dir = output_dir

        # Create output directories
        os.makedirs(os.path.join(output_dir, "benign"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "malware"), exist_ok=True)

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler("../../../Project Code Files/stuff/Image Conversion/malware_image_generation.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("MalwareImageGenerator")

    def get_opcode_embedding(self, opcode):
        """Get embedding for a single opcode, return zero vector if not in vocabulary"""
        try:
            return self.model.wv[opcode]
        except KeyError:
            return np.zeros(self.embedding_size)

    def _calculate_entropy(self, sequence, window_size=100):
        """Calculate Shannon entropy for windows of the sequence"""
        entropies = []

        # Count sequences shorter than window size
        if len(sequence) <= window_size:
            # Count frequency of each opcode
            freq_dict = {}
            for op in sequence:
                if op in freq_dict:
                    freq_dict[op] += 1
                else:
                    freq_dict[op] = 1

            # Calculate entropy
            entropy = 0
            for op, count in freq_dict.items():
                p = count / len(sequence)
                entropy -= p * math.log2(p)

            return [entropy]

        # For longer sequences, calculate rolling entropy
        for i in range(0, len(sequence) - window_size, window_size):
            window = sequence[i:i + window_size]

            # Count frequency of each opcode in the window
            freq_dict = {}
            for op in window:
                if op in freq_dict:
                    freq_dict[op] += 1
                else:
                    freq_dict[op] = 1

            # Calculate entropy for the window
            entropy = 0
            for op, count in freq_dict.items():
                p = count / window_size
                entropy -= p * math.log2(p)

            entropies.append(entropy)

        return entropies

    def _identify_rare_opcodes(self, sequence, global_frequencies):
        """Identify rare opcodes in the sequence based on global frequencies"""
        rare_indices = []

        for i, op in enumerate(sequence):
            if op in global_frequencies and global_frequencies[op] < 0.001:  # Threshold for "rare"
                rare_indices.append(i)

        return rare_indices

    def hilbert_mapping(self, opcode_sequence, global_frequencies=None):
        """
        Map opcode sequence to 2D image using Hilbert curve with hybrid sampling

        Args:
            opcode_sequence: List of opcodes
            global_frequencies: Dictionary of global opcode frequencies (optional)

        Returns:
            2D numpy array representing the image (grayscale)
        """
        # Calculate entropy across the sequence
        entropies = self._calculate_entropy(opcode_sequence)

        # Identify rare opcodes if global frequencies are provided
        rare_indices = []
        if global_frequencies:
            rare_indices = self._identify_rare_opcodes(opcode_sequence, global_frequencies)

        # Determine Hilbert curve order based on image size
        # For 256x256, order is 8 (2^8 = 256)
        order = int(math.log2(self.img_size[0]))
        n = 2 ** order
        total_pixels = n * n

        # Determine sampling strategy based on sequence length
        seq_length = len(opcode_sequence)

        # Create a sampling importance map
        importance = np.ones(seq_length)

        # Emphasize the beginning of the file (headers, entry points)
        header_size = min(1000, seq_length // 10)
        importance[:header_size] = 2.0

        # Emphasize high entropy regions
        if len(entropies) > 0:
            entropy_threshold = np.percentile(entropies, 75)
            for i, entropy in enumerate(entropies):
                if entropy > entropy_threshold:
                    start_idx = i * 100  # Based on window_size in _calculate_entropy
                    end_idx = min(start_idx + 100, seq_length)
                    importance[start_idx:end_idx] = 1.5

        # Emphasize rare opcodes
        for idx in rare_indices:
            if idx < seq_length:
                importance[idx] = 2.0

        # Normalize importance values to create a probability distribution
        importance = importance / importance.sum()

        # Sample indices based on importance
        if seq_length > total_pixels:
            sampled_indices = np.random.choice(
                seq_length,
                size=total_pixels,
                replace=False,
                p=importance
            )
            sampled_indices.sort()  # Keep in original sequence order
        else:
            # If sequence is shorter, use all opcodes and repeat if necessary
            repeats = math.ceil(total_pixels / seq_length)
            sampled_indices = np.tile(np.arange(seq_length), repeats)[:total_pixels]

        # Get embeddings for sampled opcodes
        embeddings = np.array([self.get_opcode_embedding(opcode_sequence[i]) for i in sampled_indices])

        # Apply PCA to reduce embeddings to a single dimension
        if len(embeddings) > 0 and self.embedding_size > 1:
            try:
                # Check for zero variance embeddings
                if np.all(embeddings == embeddings[0]):
                    self.logger.warning("All embeddings are identical - using random values instead")
                    grayscale_values = np.random.random(len(embeddings))
                else:
                    # Apply PCA to reduce to 1 dimension
                    pca = PCA(n_components=1, random_state=42)
                    grayscale_values = pca.fit_transform(embeddings).flatten()

                    # Normalize to 0-1 range
                    if grayscale_values.max() > grayscale_values.min():
                        grayscale_values = (grayscale_values - grayscale_values.min()) / (
                                    grayscale_values.max() - grayscale_values.min())
                    else:
                        self.logger.warning("PCA produced uniform values - using normalized embeddings instead")
                        # Use the first dimension of each embedding as fallback
                        grayscale_values = embeddings[:, 0]
                        if grayscale_values.max() > grayscale_values.min():
                            grayscale_values = (grayscale_values - grayscale_values.min()) / (
                                        grayscale_values.max() - grayscale_values.min())
                        else:
                            grayscale_values = np.linspace(0, 1, len(embeddings))
            except Exception as e:
                self.logger.warning(f"PCA failed: {str(e)} - using first dimension instead")
                # Use the first dimension of each embedding as fallback
                grayscale_values = embeddings[:, 0]
                if grayscale_values.max() > grayscale_values.min():
                    grayscale_values = (grayscale_values - grayscale_values.min()) / (
                                grayscale_values.max() - grayscale_values.min())
                else:
                    grayscale_values = np.linspace(0, 1, len(embeddings))
        else:
            # If embedding size is 1 or we have an empty array, use the embeddings directly
            if len(embeddings) > 0:
                grayscale_values = embeddings[:, 0]
                if grayscale_values.max() > grayscale_values.min():
                    grayscale_values = (grayscale_values - grayscale_values.min()) / (
                                grayscale_values.max() - grayscale_values.min())
                else:
                    grayscale_values = np.linspace(0, 1, len(embeddings))
            else:
                grayscale_values = np.array([])

        # Create empty grayscale image
        img_data = np.zeros(self.img_size)

        # Map using Hilbert curve
        for i in range(min(total_pixels, len(grayscale_values))):
            # Convert distance along curve to (x,y) coordinates
            x, y = self._d2xy(n, i)

            if x < self.img_size[0] and y < self.img_size[1]:
                img_data[y, x] = grayscale_values[i]

        # Ensure the image has some variation (not a solid color)
        if img_data.max() == img_data.min():
            self.logger.warning("Generated a solid color image - adding noise for visibility")
            # Add some random noise to create variation
            noise = np.random.normal(0, 0.1, img_data.shape)
            img_data = img_data + noise
            # Re-normalize to 0-1
            img_data = (img_data - img_data.min()) / (img_data.max() - img_data.min() + 1e-10)

        return img_data

    def _d2xy(self, n, d):
        """Convert a distance along the Hilbert curve to (x,y) coordinates"""
        x, y = 0, 0
        s = 1
        while s < n:
            rx = 1 & (d // 2)
            ry = 1 & (d ^ rx)
            x, y = self._rotate(s, x, y, rx, ry)
            x += s * rx
            y += s * ry
            d //= 4
            s *= 2
        return x, y

    def _rotate(self, n, x, y, rx, ry):
        """Helper function for Hilbert curve calculation"""
        if ry == 0:
            if rx == 1:
                x = n - 1 - x
                y = n - 1 - y
            x, y = y, x
        return x, y

    def calculate_global_frequencies(self, data_dir):
        """Calculate global opcode frequencies across all files"""
        self.logger.info("Calculating global opcode frequencies...")

        all_opcodes = []
        file_paths = self._get_all_opcode_files(data_dir)

        for file_path in tqdm(file_paths[:1000]):  # Limit to 1000 files for efficiency
            try:
                with open(file_path, 'r', errors='ignore') as f:
                    opcodes = f.read().strip().split()
                    # Sample a subset for very large files
                    if len(opcodes) > 10000:
                        indices = np.linspace(0, len(opcodes) - 1, 10000, dtype=int)
                        opcodes = [opcodes[i] for i in indices]
                    all_opcodes.extend(opcodes)
            except Exception as e:
                self.logger.warning(f"Error reading {file_path}: {str(e)}")

        # Calculate frequencies
        total_count = len(all_opcodes)
        frequencies = {}

        for op in all_opcodes:
            if op in frequencies:
                frequencies[op] += 1
            else:
                frequencies[op] = 1

        # Convert to relative frequencies
        for op in frequencies:
            frequencies[op] /= total_count

        self.logger.info(f"Calculated frequencies for {len(frequencies)} unique opcodes")
        return frequencies

    def _get_all_opcode_files(self, data_dir):
        """Get paths to all opcode files in the data directory"""
        file_paths = []

        # Process benign files
        benign_dir = os.path.join(data_dir, "benign", "extracted")
        if os.path.exists(benign_dir):
            for root, _, files in os.walk(benign_dir):
                for file in files:
                    file_paths.append(os.path.join(root, file))

        # Process malware files
        malware_dir = os.path.join(data_dir, "v077_clean")
        if os.path.exists(malware_dir):
            for root, _, files in os.walk(malware_dir):
                for file in files:
                    file_paths.append(os.path.join(root, file))

        return file_paths

    def generate_images(self, data_dir):
        """Generate images for all opcode files in the data directory"""
        self.logger.info("Starting image generation process...")

        # Calculate global opcode frequencies
        global_frequencies = self.calculate_global_frequencies(data_dir)

        # Get all opcode files
        file_paths = self._get_all_opcode_files(data_dir)
        self.logger.info(f"Found {len(file_paths)} files to process")

        stats = {"benign": 0, "malware": 0, "errors": 0}

        # Process each file
        for file_path in tqdm(file_paths):
            try:
                # Determine if benign or malware
                if "benign" in file_path:
                    label = "benign"
                else:
                    label = "malware"

                # Read opcodes
                with open(file_path, 'r', errors='ignore') as f:
                    opcodes = f.read().strip().split()

                # Skip very small files
                if len(opcodes) < 10:
                    self.logger.warning(f"Skipping {file_path} (too small: {len(opcodes)} opcodes)")
                    continue

                # Generate grayscale image
                img_data = self.hilbert_mapping(opcodes, global_frequencies)

                # Save the generated image
                self._save_image(img_data, file_path, label)
                stats[label] += 1

                # Free up memory
                del img_data, opcodes
                gc.collect()

            except Exception as e:
                self.logger.error(f"Error processing {file_path}: {str(e)}")
                stats["errors"] += 1

        self.logger.info(f"Image generation complete. Stats: {stats}")
        return stats

    def visualize_sample_images(self, num_samples=5):
        """Visualize sample generated images for both classes"""
        fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))

        # Get sample images for benign
        benign_files = glob.glob(os.path.join(self.output_dir, "benign", "*.png"))
        if len(benign_files) > num_samples:
            benign_files = np.random.choice(benign_files, num_samples, replace=False)

        # Get sample images for malware
        malware_files = glob.glob(os.path.join(self.output_dir, "malware", "*.png"))
        if len(malware_files) > num_samples:
            malware_files = np.random.choice(malware_files, num_samples, replace=False)

        # Display benign samples
        for i, file_path in enumerate(benign_files):
            if i < num_samples:
                img = Image.open(file_path)
                axes[0, i].imshow(img, cmap='gray')
                axes[0, i].set_title(f"Benign {i + 1}")
                axes[0, i].axis('off')

        # Display malware samples
        for i, file_path in enumerate(malware_files):
            if i < num_samples:
                img = Image.open(file_path)
                axes[1, i].imshow(img, cmap='gray')
                axes[1, i].set_title(f"Malware {i + 1}")
                axes[1, i].axis('off')

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, "sample_images.png"))
        plt.close()

    def visualize_embedding_space(self, num_opcodes=100):
        """Visualize the embedding space of most common opcodes"""
        # Get most common opcodes from the model's vocabulary
        vocab = self.model.wv.index_to_key[:num_opcodes]

        # Get embeddings for these opcodes
        embeddings = np.array([self.model.wv[word] for word in vocab])

        # Apply dimensionality reduction for visualization using UMAP
        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)
        embeddings_2d = reducer.fit_transform(embeddings)

        # Create a DataFrame for easier plotting
        df = pd.DataFrame({
            'opcode': vocab,
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1]
        })

        # Create the plot
        plt.figure(figsize=(12, 10))
        plt.scatter(df['x'], df['y'], alpha=0.7)

        # Add opcode labels to some points
        for i, row in df.sample(min(30, len(df))).iterrows():
            plt.annotate(row['opcode'], (row['x'], row['y']))

        plt.title('UMAP visualization of opcode embeddings')
        plt.xlabel('t-SNE dimension 1')
        plt.ylabel('t-SNE dimension 2')
        plt.savefig(os.path.join(self.output_dir, "embedding_space.png"))
        plt.close()

    def _save_image(self, img_data, file_path, label):
        """Helper method to save a grayscale image"""
        # Convert to uint8 for saving
        img_data_uint8 = (img_data * 255).astype(np.uint8)
        img = Image.fromarray(img_data_uint8, 'L')  # 'L' mode for grayscale

        # Create output filename
        file_name = os.path.basename(file_path)
        output_path = os.path.join(self.output_dir, label, f"{file_name}.png")

        # Save image
        img.save(output_path)

        return output_path

    def visualize_hilbert_curve(self):
        """Visualize the Hilbert curve mapping process"""
        order = int(math.log2(self.img_size[0]))
        n = 2 ** order

        points = []
        for i in range(n * n):
            x, y = self._d2xy(n, i)
            points.append((x, y))

        # Convert to numpy array for easier manipulation
        points = np.array(points)

        # Create a colormap based on position along the curve
        colors = np.linspace(0, 1, len(points))

        plt.figure(figsize=(10, 10))
        plt.scatter(points[:, 0], points[:, 1], c=colors, cmap='viridis', s=1)
        plt.title(f'Hilbert Curve (Order {order})')
        plt.xlim(0, n)
        plt.ylim(0, n)
        plt.savefig(os.path.join(self.output_dir, "hilbert_curve.png"))
        plt.close()


# Example usage
if __name__ == "__main__":
    # Path to the trained word2vec model
    model_path = "path/to/your/word2vec/model.bin"

    # Initialize the generator
    generator = MalwareImageGenerator(model_path, output_dir="./malware_images")

    # Generate images
    stats = generator.generate_images("./Data")

    # Create visualizations
    generator.visualize_sample_images()
    generator.visualize_embedding_space()
    generator.visualize_hilbert_curve()

    print(f"Image generation complete. Stats: {stats}")